{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aeaff6-74ad-4c7e-99dc-ef65c3146bbf",
   "metadata": {},
   "source": [
    "# Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19b6433f-cff7-4776-91d2-ffc21cee43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import joblib\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcff27-c306-4bec-8c97-7dbe6dbd4961",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b0a838e-7311-4f81-9cc2-7b9e8ef1b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this accordingly\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "dataset_dir = f\"{project_path}/scicite_preprocessed\"\n",
    "results_dir = f\"{project_path}/results\"\n",
    "\n",
    "dataset = \"selected-features\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00321b-7a8c-4234-9db9-c7e649dd9355",
   "metadata": {},
   "source": [
    "## 1. Load database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435fdbbf-06b9-4eb6-be18-5f8980a4ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_df = pd.read_json(f\"{dataset_dir}/train_background.jsonl\", lines=True)\n",
    "method_df = pd.read_json(f\"{dataset_dir}/train_method.jsonl\", lines=True)\n",
    "result_df = pd.read_json(f\"{dataset_dir}/train_result.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac257502-7fe9-4bea-93c6-a9ff0a10db1a",
   "metadata": {},
   "source": [
    "## 2. Create BM25, Semantic and Hybrid Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ec8782-4669-4de6-bdc2-3887b160b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRetriever:\n",
    "    def __init__(self, documents, paper_ids, model_name=\"allenai/scibert_scivocab_uncased\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        A Semantic Retriever using SciBERT embeddings for short documents (citations).\n",
    "        Stores document-paper_id pairs for retrieval.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "\n",
    "        # Load SciBERT tokenizer & model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "        # Store (document, paper_id) pairs\n",
    "        self.documents = documents\n",
    "        self.paper_ids = paper_ids\n",
    "        self.embeddings = self.embed_documents(documents)\n",
    "\n",
    "    def embed_text(self, text, max_length=512):\n",
    "        \"\"\" Converts text into SciBERT embeddings. \"\"\"\n",
    "        tokens = self.tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokens)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()  # [CLS] token\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        \"\"\" Embeds all short documents using SciBERT. \"\"\"\n",
    "        return np.array([self.embed_text(doc) for doc in documents])\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        \"\"\" Retrieves the most relevant documents along with their paper IDs. \"\"\"\n",
    "        query_embedding = self.embed_text(query).reshape(1, -1)\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings).flatten()\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]  # Get top-k sorted indices\n",
    "        return [(self.documents[i], self.paper_ids[i]) for i in top_indices]\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, documents, paper_ids):\n",
    "        \"\"\"\n",
    "        A BM25 Retriever using term frequency and inverse document frequency scores.\n",
    "        Stores document-paper_id pairs for retrieval.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.paper_ids = paper_ids\n",
    "\n",
    "        self.tokenized_docs = [word_tokenize(doc.lower()) for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        \n",
    "        query_tokens = word_tokenize(query.lower())\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "\n",
    "        return [(self.documents[i], self.paper_ids[i]) for i in top_indices]\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, documents, paper_ids, bm25_weight=0.5, semantic_weight=0.5, model_name=\"allenai/scibert_scivocab_uncased\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        HybridRetriever: Combines BM25 and Semantic Retrieval.\n",
    "        Uses a weighted combination of both scores to retrieve documents.\n",
    "        \"\"\"\n",
    "        self.bm25_weight = bm25_weight\n",
    "        self.semantic_weight = semantic_weight\n",
    "\n",
    "        self.documents = documents\n",
    "        self.paper_ids = paper_ids\n",
    "        \n",
    "        # BM25\n",
    "        self.tokenized_docs = [word_tokenize(doc.lower()) for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "\n",
    "        # Semantic\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
    "        self.embeddings = self.embed_documents(documents)\n",
    "    \n",
    "    def embed_text(self, text, max_length=512):\n",
    "        \"\"\" Converts text into SciBERT embeddings. \"\"\"\n",
    "        tokens = self.tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokens)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()  # [CLS] token\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        \"\"\" Embeds all short documents using SciBERT. \"\"\"\n",
    "        return np.array([self.embed_text(doc) for doc in documents])\n",
    "    \n",
    "    def retrieve(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieves the top_k most relevant documents using a combination of BM25 and semantic scores.\n",
    "        \"\"\"\n",
    "        # BM25\n",
    "        query_tokens = word_tokenize(query.lower())\n",
    "        bm25_scores = self.bm25.get_scores(query_tokens)\n",
    "\n",
    "        # Semantic\n",
    "        query_embedding = self.embed_text(query).reshape(1, -1)\n",
    "        semantic_scores = cosine_similarity(query_embedding, self.embeddings).flatten()\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined_scores = (self.bm25_weight * bm25_scores) + (self.semantic_weight * semantic_scores)\n",
    "\n",
    "        # Retrieve top-k sentences\n",
    "        top_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n",
    "        \n",
    "        return [(self.documents[i], self.paper_ids[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790ba1b1-086b-4d37-b496-e21057d6671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract citations & paper IDs for each label\n",
    "method_docs, method_ids = method_df[\"string\"].tolist(), method_df[\"id\"].tolist()\n",
    "background_docs, background_ids = background_df[\"string\"].tolist(), background_df[\"id\"].tolist()\n",
    "result_docs, result_ids = result_df[\"string\"].tolist(), result_df[\"id\"].tolist()\n",
    "\n",
    "# Initialize retrievers for each category\n",
    "bm_method_retriever = BM25Retriever(method_docs, method_ids)\n",
    "bm_background_retriever = BM25Retriever(background_docs, background_ids)\n",
    "bm_result_retriever = BM25Retriever(result_docs, result_ids)\n",
    "\n",
    "sem_method_retriever = SemanticRetriever(method_docs, method_ids)\n",
    "sem_background_retriever = SemanticRetriever(background_docs, background_ids)\n",
    "sem_result_retriever = SemanticRetriever(result_docs, result_ids)\n",
    "\n",
    "hyb_method_retriever = HybridRetriever(method_docs, method_ids)\n",
    "hyb_background_retriever = HybridRetriever(background_docs, background_ids)\n",
    "hyb_result_retriever = HybridRetriever(result_docs, result_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74a726-8550-48ab-9192-7631e51d718a",
   "metadata": {},
   "source": [
    "## 3. Classify input queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f7c7693-4384-45c3-9ba9-d44f446869cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_test_df = pd.read_json(f\"{project_path}/scicite/test.jsonl\", lines=True)\n",
    "test_df = pd.read_csv(f\"{dataset_dir}/test-{dataset}.csv\")\n",
    "\n",
    "sample_test_df = test_df.sample(n=100, random_state=42)\n",
    "X_test = sample_test_df.drop(columns=['label'])\n",
    "y_test = sample_test_df[\"label\"]\n",
    "# X_test = test_df.drop(columns=['label'])\n",
    "# y_test = test_df[\"label\"]\n",
    "\n",
    "# Select the same indices from the original test dataset\n",
    "ori_sample_test_df = ori_test_df.loc[sample_test_df.index]\n",
    "\n",
    "# Display the selected rows\n",
    "# display(X_test)\n",
    "# display(ori_sample_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d42954b3-80dd-4ab8-9f90-53a8da5960ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifier \n",
    "classifier = joblib.load(f\"{dataset_dir}/selected_classifier.pkl\")\n",
    "\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "# Print predictions and true labels\n",
    "# print(\"Predictions vs True Labels:\")\n",
    "# df_comparison = pd.DataFrame({\"Predicted\": pred, \"True\": y_test.values})\n",
    "# print(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d882a-c4b2-4b6a-b6ac-a7cd59d68bc1",
   "metadata": {},
   "source": [
    "## 4. Retrieve similar citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe433d25-3332-4f36-b58d-a3297edd293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to /home/brina/nus-mcomp/sem3/cs4248-natural-language-processing/Project/CS4248-NLP-Project/results\n"
     ]
    }
   ],
   "source": [
    "# Initialize retrievers for each label\n",
    "label_encoder = joblib.load(f\"{dataset_dir}/label_encoder.pkl\")\n",
    "label_strings = label_encoder.inverse_transform(pred)\n",
    "\n",
    "bm_retrievers = {\n",
    "    \"background\": bm_background_retriever,\n",
    "    \"method\": bm_method_retriever,\n",
    "    \"result\": bm_result_retriever\n",
    "}\n",
    "\n",
    "sem_retrievers = {\n",
    "    \"background\": sem_background_retriever,\n",
    "    \"method\": sem_method_retriever,\n",
    "    \"result\": sem_result_retriever\n",
    "}\n",
    "\n",
    "hyb_retrievers = {\n",
    "    \"background\": hyb_background_retriever,\n",
    "    \"method\": hyb_method_retriever,\n",
    "    \"result\": hyb_result_retriever\n",
    "}\n",
    "\n",
    "# Initialize empty lists to hold the results for each retriever type\n",
    "bm_retrieved_docs = []\n",
    "sem_retrieved_docs = []\n",
    "hyb_retrieved_docs = []\n",
    "\n",
    "for idx, label in enumerate(label_strings):\n",
    "    query = ori_sample_test_df.iloc[idx][\"string\"]\n",
    "    bm_retriever = bm_retrievers.get(label)\n",
    "    sem_retriever = sem_retrievers.get(label)\n",
    "    hyb_retriever = hyb_retrievers.get(label)\n",
    "\n",
    "    # BM Retriever\n",
    "    relevant_docs = bm_retriever.retrieve(query, top_k=3)\n",
    "    bm_results = []\n",
    "    for doc, paper_id in relevant_docs:\n",
    "        bm_results.append({\"Document\": doc, \"Paper ID\": paper_id})\n",
    "\n",
    "    bm_retrieved_docs.append({\n",
    "        \"Query\": query,\n",
    "        \"Predicted label\": label,\n",
    "        \"Retrieved docs\": bm_results\n",
    "    })\n",
    "    \n",
    "    # Semantic Retriever\n",
    "    relevant_docs = sem_retriever.retrieve(query, top_k=3)\n",
    "    sem_results = []\n",
    "    for doc, paper_id in relevant_docs:\n",
    "        sem_results.append({\"Document\": doc, \"Paper ID\": paper_id})\n",
    "        \n",
    "    sem_retrieved_docs.append({\n",
    "        \"Query\": query,\n",
    "        \"Predicted label\": label,\n",
    "        \"Retrieved docs\": sem_results\n",
    "    })\n",
    "    \n",
    "    # Hybrid Retriever\n",
    "    relevant_docs = hyb_retriever.retrieve(query, top_k=3)\n",
    "    hyb_results = []\n",
    "    for doc, paper_id in relevant_docs:\n",
    "        hyb_results.append({\"Document\": doc, \"Paper ID\": paper_id})\n",
    "\n",
    "    hyb_retrieved_docs.append({\n",
    "        \"Query\": query,\n",
    "        \"Predicted label\": label,\n",
    "        \"Retrieved docs\": hyb_results\n",
    "    })\n",
    "    \n",
    "# Save the retrieved documents for each retriever to a separate JSON file\n",
    "with open(f\"{results_dir}/bm_retrieved_docs.json\", \"w\") as f:\n",
    "    json.dump(bm_retrieved_docs, f, indent=4)\n",
    "\n",
    "with open(f\"{results_dir}/sem_retrieved_docs.json\", \"w\") as f:\n",
    "    json.dump(sem_retrieved_docs, f, indent=4)\n",
    "\n",
    "with open(f\"{results_dir}/hyb_retrieved_docs.json\", \"w\") as f:\n",
    "    json.dump(hyb_retrieved_docs, f, indent=4)\n",
    "\n",
    "print(f\"Saved results to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40028d69-eb9b-4c96-be03-28f43e769780",
   "metadata": {},
   "source": [
    "## 5. Evaluate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a7fa062-aac0-458c-8498-07917a06be1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to /home/brina/nus-mcomp/sem3/cs4248-natural-language-processing/Project/CS4248-NLP-Project/results\n"
     ]
    }
   ],
   "source": [
    "api_key = \"fy3jHNMV7OC7t7fQprQkFgp7NeSlRsMG\"\n",
    "base_url = \"https://api.deepinfra.com/v1/openai\"\n",
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "# model = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "\n",
    "llm = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "# Function to call OpenAI and get relevance feedback (qualitative)\n",
    "def evaluate_retrieval(query, retrieved_docs):\n",
    "    # Prepare the prompt for OpenAI to evaluate the relevance\n",
    "\n",
    "    prompt = \"\"\"\n",
    "                You are an evaluator tasked to determine whether each document is relevant to the citation query.\n",
    "                For each document, return `1` if it is relevant to the citation, or `0` if irrelevant.\n",
    "                Directly return the answer as `Answer: ` with no explanation.\n",
    "                It should be a sequence of digits separated by spaces, with one digit per document and in the same order as presented.\n",
    "\n",
    "                For example,\n",
    "                Citation Query: \"We adopt the method proposed by Smith et al. (2020) for neural text generation, which introduces a variational decoding strategy.\"  \n",
    "                Document 1: \"Smith et al. (2020) propose a variational decoding framework that improves diversity in neural text generation.\"  \n",
    "                Document 2: \"Brown et al. (2019) study reinforcement learning for policy optimization in robotics tasks.\"  \n",
    "                Document 3: \"The paper explores attention mechanisms in neural networks but does not mention variational decoding.\"\n",
    "                Answer: 1 0 0\n",
    "                \n",
    "              \"\"\"\n",
    "    \n",
    "    prompt += f\"Citation Query: {query}\\n\"\n",
    "                \n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        prompt += f\"Document {i + 1}: {doc['Document']}\\n\"\n",
    "\n",
    "    prompt += \"Answer: \"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Send the prompt to OpenAI's API\n",
    "    response = llm.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=200\n",
    "    )\n",
    "\n",
    "    # Extract the digit sequence from the response\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    # print(result)\n",
    "\n",
    "    match = re.search(r\"Answer:\\s*(.*)\", result)\n",
    "    if match:\n",
    "        digit_sequence = match.group(1)\n",
    "    else:\n",
    "        # If \"Answer:\" is not found, assume the model just returns the digits directly\n",
    "        digit_sequence = result\n",
    "\n",
    "    # Split and validate\n",
    "    relevance_scores = digit_sequence.strip().split()\n",
    "    \n",
    "    # Ensure that relevance_scores only contain 0s and 1s, and match the length of retrieved_docs\n",
    "    relevance_scores = [score if score in ['0', '1'] else '0' for score in relevance_scores]\n",
    "    \n",
    "    # If there are fewer relevance scores than retrieved docs, append 0s\n",
    "    if len(relevance_scores) < len(retrieved_docs):\n",
    "        relevance_scores += ['0'] * (len(retrieved_docs) - len(relevance_scores))\n",
    "    \n",
    "    # If there are more relevance scores than retrieved docs, truncate the extra scores\n",
    "    elif len(relevance_scores) > len(retrieved_docs):\n",
    "        relevance_scores = relevance_scores[:len(retrieved_docs)]\n",
    "\n",
    "    # Package results\n",
    "    evaluated_docs = []\n",
    "    for doc, relevance in zip(retrieved_docs, relevance_scores):\n",
    "        evaluated_docs.append({\n",
    "            \"Document\": doc['Document'],\n",
    "            \"Paper ID\": doc['Paper ID'],\n",
    "            \"Relevance\": int(relevance)\n",
    "        })\n",
    "\n",
    "    return evaluated_docs\n",
    "\n",
    "# Load the retrieved documents JSON files\n",
    "with open(f\"{results_dir}/bm_retrieved_docs.json\", \"r\") as f:\n",
    "    bm_retrieved_docs = json.load(f)\n",
    "\n",
    "with open(f\"{results_dir}/sem_retrieved_docs.json\", \"r\") as f:\n",
    "    sem_retrieved_docs = json.load(f)\n",
    "\n",
    "with open(f\"{results_dir}/hyb_retrieved_docs.json\", \"r\") as f:\n",
    "    hyb_retrieved_docs = json.load(f)\n",
    "\n",
    "# Initialize lists to store evaluations\n",
    "bm_evaluations = []\n",
    "sem_evaluations = []\n",
    "hyb_evaluations = []\n",
    "\n",
    "# Evaluate each query for BM retriever\n",
    "for query_data in bm_retrieved_docs:\n",
    "    query = query_data['Query']\n",
    "    retrieved_docs = query_data['Retrieved docs']\n",
    "    evaluated_docs = evaluate_retrieval(query, retrieved_docs)\n",
    "    bm_evaluations.append({\n",
    "        \"Query\": query,\n",
    "        \"Predicted label\": query_data['Predicted label'],\n",
    "        \"Retrieved docs\": evaluated_docs\n",
    "    })\n",
    "\n",
    "# Evaluate each query for Semantic retriever\n",
    "for query_data in sem_retrieved_docs:\n",
    "    query = query_data['Query']\n",
    "    retrieved_docs = query_data['Retrieved docs']\n",
    "    evaluated_docs = evaluate_retrieval(query, retrieved_docs)\n",
    "    sem_evaluations.append({\n",
    "        \"Query\": query,\n",
    "        \"Predicted label\": query_data['Predicted label'],\n",
    "        \"Retrieved docs\": evaluated_docs\n",
    "    })\n",
    "\n",
    "# Evaluate each query for Hybrid retriever\n",
    "for query_data in hyb_retrieved_docs:\n",
    "    query = query_data['Query']\n",
    "    retrieved_docs = query_data['Retrieved docs']\n",
    "    evaluated_docs = evaluate_retrieval(query, retrieved_docs)\n",
    "    hyb_evaluations.append({\n",
    "        \"Query\": query,\n",
    "        \"Predicted label\": query_data['Predicted label'],\n",
    "        \"Retrieved docs\": evaluated_docs\n",
    "    })\n",
    "\n",
    "# Save the evaluations to JSON files\n",
    "with open(f\"{results_dir}/bm_evaluations.json\", \"w\") as f:\n",
    "    json.dump(bm_evaluations, f, indent=4)\n",
    "\n",
    "with open(f\"{results_dir}/sem_evaluations.json\", \"w\") as f:\n",
    "    json.dump(sem_evaluations, f, indent=4)\n",
    "\n",
    "with open(f\"{results_dir}/hyb_evaluations.json\", \"w\") as f:\n",
    "    json.dump(hyb_evaluations, f, indent=4)\n",
    "\n",
    "print(f\"Saved results to {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5e3cb92-de3e-47ef-964d-be4fadecfa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Precision Scores ===\n",
      "BM Precision: 0.4033\n",
      "Semantic Precision: 0.3467\n",
      "Hybrid Precision: 0.3700\n"
     ]
    }
   ],
   "source": [
    "def compute_precision(evaluations):\n",
    "    total_docs = 0\n",
    "    total_relevant = 0\n",
    "    for query_eval in evaluations:\n",
    "        for doc in query_eval['Retrieved docs']:\n",
    "            total_docs += 1\n",
    "            if doc['Relevance'] == 1:\n",
    "                total_relevant += 1\n",
    "    precision = total_relevant / total_docs if total_docs else 0\n",
    "    return precision\n",
    "\n",
    "bm_precision = compute_precision(bm_evaluations)\n",
    "sem_precision = compute_precision(sem_evaluations)\n",
    "hyb_precision = compute_precision(hyb_evaluations)\n",
    "\n",
    "print(\"\\n=== Precision Scores ===\")\n",
    "print(f\"BM Precision: {bm_precision:.4f}\")\n",
    "print(f\"Semantic Precision: {sem_precision:.4f}\")\n",
    "print(f\"Hybrid Precision: {hyb_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e160a-3412-4b2d-bb6b-9dc5d7dbd24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
